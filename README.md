<h1 align="center">Hi, I'm Bavantha ğŸ‘‹</h1>
<p align="center">
  <em>Robotics researcher & doctoral candidate @ University of Twente building autonomous systems with monocular perception, deep learning, and reinforcement learning.</em>
</p>

<p align="center">
  <a href="mailto:bavanthaU@eng.pdn.ac.lk"><img src="https://img.shields.io/badge/Email-bavanthaU%40eng.pdn.ac.lk-blue?style=for-the-badge" alt="Email Bavantha"></a>
  <a href="mailto:b.udugama@utwente.nl"><img src="https://img.shields.io/badge/Academic%20Email-b.udugama%40utwente.nl-orange?style=for-the-badge" alt="Email Bavantha"></a>
</p>

---

### ğŸ‘¨â€ğŸ”¬ About Me
- Robotics researcher advancing monocular spatial perception, deep learning, and deep reinforcement learning for resilient autonomy.
- Building autonomy stacks that rely on lightweight sensor suitesâ€”RGB cameras and IMUsâ€”without sacrificing robustness or reliability.
- Sharing open-source tooling, reproducible experiments, and research companions from my doctoral work.

### ğŸ§­ What I'm Focused On Right Now
- Deploying monocular spatial systems like `mono_hydra` and `M2H` from lab conditions to real-world environments.
- Designing deep reinforcement learning policies that stay reliable under uncertainty and limited sensing.
- Contributing to collaborative robotics initiatives and student mentorship.

### ğŸ”¬ Research Spotlight
| Project | Objective | Current focus |
| --- | --- | --- |
| **mono_hydra** | Real-time monocular spatial perception system that fuses a camera and IMU to build 3D scene graphs with sub-20â€¯cm error at 15â€¯fps on laptop GPUs. | Extending from indoor pilots to outdoor scenarios, tightening the SLAM-to-scene-graph loop, and packaging a reproducible release for the robotics community. |
| **M2H (Multi-tasking Network)** | Lightweight ViT (DINOv2) multi-task framework predicting semantics, depth, edges, and normals via window-based cross-task attention. | Optimising edge deployments, broadening benchmarks (NYUDv2, Hypersim, Cityscapes), and serving as the perception backbone for mono_hydra. |

- **mono_hydra** pushes monocular SLAM beyond geometry by layering semantic understanding, enabling agile autonomy from simple sensor rigs.
- **M2H** delivers consistent cross-task predictions while staying real-time, outperforming single-task baselines and previous multi-task models across standard datasets.

### ğŸ§  Technical Interests
- Deep learning for spatial perception, multi-modal fusion, and cross-task representation learning.
- Deep reinforcement learning that keeps autonomous robots reliable in uncertain, dynamic environments.
- Monocular spatial systems like `mono_hydra` and `M2H` that turn research breakthroughs into deployable autonomy stacks.

### ğŸ› ï¸ Tools & Technologies
<p>
  <img src="https://img.shields.io/badge/ROS-22314E?style=for-the-badge&logo=ros&logoColor=white" alt="ROS" />
  <img src="https://img.shields.io/badge/ROS2-0A1E5A?style=for-the-badge&logo=ros&logoColor=white" alt="ROS2" />
  <img src="https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white" alt="Python" />
  <img src="https://img.shields.io/badge/C++-00599C?style=for-the-badge&logo=cplusplus&logoColor=white" alt="C++" />
  <img src="https://img.shields.io/badge/Pytorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white" alt="PyTorch" />
  <img src="https://img.shields.io/badge/OpenCV-5C3EE8?style=for-the-badge&logo=opencv&logoColor=white" alt="OpenCV" />
  <img src="https://img.shields.io/badge/Gazebo-4A6DBF?style=for-the-badge&logo=ros&logoColor=white" alt="Gazebo" />
  <img src="https://img.shields.io/badge/Isaac%20Sim-76B900?style=for-the-badge&logo=nvidia&logoColor=white" alt="Isaac Sim" />
  <img src="https://img.shields.io/badge/Isaac%20Lab-76B900?style=for-the-badge&logo=nvidia&logoColor=white" alt="Isaac Lab" />
  <img src="https://img.shields.io/badge/Stable--Baselines3-000000?style=for-the-badge&logo=openai&logoColor=white" alt="Stable-Baselines3" />
</p>

### ğŸ“Š Snapshot
<p align="center">
  <img src="https://github-readme-stats.vercel.app/api?username=BavanthaU&show_icons=true&count_private=true&theme=tokyonight" alt="GitHub stats" />
  <img src="https://github-readme-streak-stats.herokuapp.com/?user=BavanthaU&theme=tokyonight" alt="GitHub streak" />
</p>
<p align="center">
  <img src="./assets/current_year_commits.svg" alt="Commits this year" />
</p>

### ğŸ“ Latest Work
- <img src="https://img.shields.io/badge/IROS-2025-blue?style=flat-square" alt="IROS 2025 badge" /> **M2H: Multi-Task Learning with Efficient Window-Based Cross-Task Attention for Monocular Spatial Perception** â€“ [Paper](https://arxiv.org/abs/2510.17363) *(International Conference on Intelligent Robots and Systems, 2025)*
- <img src="https://img.shields.io/badge/GSW-2023-blue?style=flat-square" alt="GSW23 badge" /> **mono_hydra: Real-time 3D Scene Graph Construction from Monocular Camera Input with IMU** â€“ [Paper](https://arxiv.org/abs/2308.05515) *(ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, 2023)*

### ğŸ¤ Let's Collaborate
- Always open to joint projects around autonomous exploration, robust slam systems, and algorithm benchmarking.
- Reach out via email for research collaborations, talks, or mentorship opportunities.
- Follow my GitHub activity to stay updated with the latest experimental releases and robotics tooling.

---

<!---
BavanthaU/BavanthaU is a âœ¨ special âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.
You can click the Preview link to take a look at your changes.
-->
